#LDA config file.  
#See notes above options for details. 

[general]
#sets the name used in log file name to differentiate this run from another - is used to generate log filename
id=bech100
#location of data to be processed
data=/data/bechtel_flat/
#type of model - only LDA is fully supported, HDP also works, but only writes topics to the logfile at present
modeltype=LDA
#where to write the log file
logpath=.

[corpusbuild]
#all options specific to corpus building are here. Some shared options are in the general section.
corpus=/home/herrjr/dev/bech/bech.cor

#min word length - all other words dropped from corpus and documents for inference
minwordlength=3

#file must contain phrases, one to a line. whole phrase must be contained by line read from document. test is case insensitivie. phrases wrapped to a new line will not be detected.
usephrasetable=True
phrasetablefile=/usr/src/topicmodel/phrasetable.cds.txt

#space separated list of words to be dropped during corpus build and inference. Words are lowercased at runtime to match tokenization of document words
usestopwordlis=True
stopwordfile=/usr/src/topicmodel/stopwords.cds.txt
#feature size - take only the top [dictionary_size] words, in terms of corpus frequency
dictionary_size=100000

[files]
#note - if you are generating these files, this is where they will be generated. If you are using them(say during inference) this is where they will be read from.
corpus=/home/herrjr/dev/bech/bech.cor
dictionary=/home/herrjr/dev/bech/bech.dict
corpusmatrix=/home/herrjr/dev/bech/bech.mm
model=/home/herrjr/dev/bech/bech.100.lda
inferenceoutput=/home/herrjr/dev/bech/bech.100.docs.topics
tfidfmodel=/home/herrjr/dev/bech/bech.tfidf.mm

[modelgeneration]
passes=1
topics=100
#number of words displayed in each topic
topnwords=20	
#if true, a pyro4 nameserver, dispatcher and 1 or more workers must be running. see readme. 
distributed=True
#set to true to perform a single pass, but update model once every [updateevery*chunksize] documents. Recommend updateevery=1 and chunksize=10000 for million+ doc corpora
#online may prove faster will good topics and good convergence if topic drift is minimal. Note, passes is implicitly 1 when online=True. 
online=True

#how many documents to load into memory
chunksize=10000
#how many chunks to process before performing an update (the maximization step of em)
#note that if distributed is true, we will only update [workercount*update_every*chunksize] documents. 
#the more updates we perform while building a model, the more likely we will reach convergence and have good topics.
update_every=2



