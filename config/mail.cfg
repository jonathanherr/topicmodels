#LDA config file.  
#See notes above options for details. 

[general]
#sets the name used in log file name to differentiate this run from another - is used to generate log filename
id=graymail_body
#location of data to be processed
data=graymail/emails/
#type of model - only LDA is fully supported, HDP also works, but only writes topics to the logfile at present
modeltype=LDA
#where to write the log file
logpath=.

numInferenceProcesses=8
[corpusbuild]
#all options specific to corpus building are here. Some shared options are in the general section.
corpus=models/mail/bodies.cor

#min word length - all other words dropped from corpus and documents for inference
minwordlength=3

#file must contain phrases, one to a line. whole phrase must be contained by line read from document. test is case insensitivie. phrases wrapped to a new line will not be detected.
usephrasetable=False
phrasetablefile=/usr/src/topicmodel/phrasetable.cds.txt

#space separated list of words to be dropped during corpus build and inference. Words are lowercased at runtime to match tokenization of document words
usestopwordlist=False
stopwordfile=/usr/src/topicmodel/stopwords.cds.txt
#feature size - take only the top [dictionary_size] words, in terms of corpus frequency
dictionary_size=100000

[files]
#note - if you are generating these files, this is where they will be generated. If you are using them(say during inference) this is where they will be read from.
corpus=models/mail/bodies.cor
dictionary=models/mail/bodies.dict
corpusmatrix=models/mail/bodies.mm
model=models/mail/bodies.100.lda
inferenceoutput=models/mail/bodies.100.docs.topics
tfidfmatrix=models/mail/bodies.tfidf.mm
tfidfmodel=models/mail/bodies.tfidf
[modelgeneration]
passes=50
topics=10
#number of words displayed in each topic
topnwords=20	
#if true, a pyro4 nameserver, dispatcher and 1 or more workers must be running. see readme. 
distributed=False
#set to true to perform a single pass, but update model once every [updateevery*chunksize] documents. Recommend updateevery=1 and chunksize=10000 for million+ doc corpora
#online may prove faster will good topics and good convergence if topic drift is minimal. Note, passes is implicitly 1 when online=True. 
online=False
no_above=.85
no_below=.05
dictionary_size=100000

#how many documents to load into memory
chunksize=10000
#how many chunks to process before performing an update (the maximization step of em)
#note that if distributed is true, we will only update [workercount*update_every*chunksize] documents. 
#the more updates we perform while building a model, the more likely we will reach convergence and have good topics.
update_every=2

[rules]
#rules used when generating corpus file and when reading files for inference

#some of these(marked) are currently on by default(and aren't affected by these options yet),
#so this is a definition of hardcoded rules right now
#default
minwordlength=3
#drop phrases in table from corpus
usephrasetable=False
#file must contain phrases, one to a line. whole phrase must be contained by line read from document. test is case insensitivie. phrases wrapped to a new line will not be detected.
phrasetablefile=/usr/src/topicmodel/phrasetable.cds.txt
#default
usestopwordlist=False
stopwordfile=/usr/src/topicmodel/stopwords.cds.txt
#default
ignoreUniqueWords=False
#default
allowemailaddresses=False
#default
allowurls=False
#default - skip obvious email headings like "To: xxx" "From: xxx" etc.
allowemailheaders=False
useregex=False
regexfile=""

