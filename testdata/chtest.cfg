#LDA config file.  
#See notes above options for details. 

[general]
#sets the name used in log file name to differentiate this run from another - is used to generate log filename
id=chi_test
#location of data to be processed
data=testdata/corpora/chinese
#type of model - only LDA is fully supported, HDP also works, but only writes topics to the logfile at present
modeltype=LDA
#where to write the log file
logpath=.
#number of processes to start when running inference
numInferenceProcesses=4
[rules]
#min word length - all other words dropped from corpus and documents for inference
minwordlength=3

#file must contain phrases, one to a line. whole phrase must be contained by line read from document. test is case insensitivie. phrases wrapped to a new line will not be detected.
usephrasetable=true
phrasetablefile=testdata/phrasetable.txt
#set to true and assing a path to turn on testing for regex in coprus files while creating .cor file. 
useregex=true
regexfile=testdata/regex.txt
#space separated list of words to be dropped during corpus build and inference. Words are lowercased at runtime to match tokenization of document words
usestopwordlist=true
stopwordfile=testdata/stopwords.txt
#number of processes to start when running corpus bulid
numBuildProcesses=1
#remove words used only once
removeOnceWords=true
[files]
#note - if you are generating these files, this is where they will be generated. If you are using them(say during inference) this is where they will be read from.
corpus=testdata/output/chi_test.cor
dictionary=testdata/output/chi_test.dict
corpusmatrix=testdata/output/chi_test.mm
model=testdata/output/chi_test.100.lda
inferenceoutput=testdata/output/chi_test.100.docs.topics
tfidfmodel=testdata/output/chi_test.tfidf
tfidfmatrix=testdata/output/chi_test.tfidf.mm

[modelgeneration]
passes=1
topics=10
#number of words displayed in each topic
topnwords=20	
#if true, a pyro4 nameserver, dispatcher and 1 or more workers must be running. see readme. 
distributed=false
#set to true to perform a single pass, but update model once every [updateevery*chunksize] documents. Recommend updateevery=1 and chunksize=10000 for million+ doc corpora 

#online may prove faster with good topics and good convergence if topic drift is minimal. Note, passes is implicitly 1 when online=True. 
online=true

#feature size - take only the top [dictionary_size] words, in terms of corpus frequency
dictionary_size=100000
#filter out tokens that ppear in more than no_above documents(fratcion of total corpus size)
no_above=1.0
#filter out tokens that ppear in less than this number of documents
no_below=1
#how many documents to load into memory
chunksize=10000
#how many chunks to process before performing an update (the maximization step of em)
#note that if distributed is true, we will only update [workercount*update_every*chunksize] documents. 
#the more updates we perform while building a model, the more likely we will reach convergence and have good topics.
update_every=2



